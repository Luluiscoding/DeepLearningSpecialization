{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep L-layer Neural Network\n",
    "Here are examples about shallow and deep NN:  \n",
    "<img src=\"images/shallowdeep.png\" width=\"600\">\n",
    "\n",
    "Some notations for deep NN:  \n",
    "<img src=\"images/4layerNN.png\" width=\"400\">  \n",
    "$L$: number of layers.  \n",
    "$n^{[l]}:$ number of units in layer $l$, in the example, $n^{[0]}=n_x=3$, $n^{[1]}=n^{[2]}=5$, $n^{[3]}=3$, $n^{[4]}=n^{[L]}=1=\\hat{y}$.  \n",
    "$a^{[l]}:$ activation function in layer $l$, $x=a^{[0]}$.  \n",
    "$w^{[l]}:$ weights for computing $z^{[l]}$.  \n",
    "$b^{[l]}:$ bias for computing $z^{[l]}$.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation in a Deep Network  \n",
    "<img src=\"images/4layerNN.png\" width=\"400\">    \n",
    "\n",
    "In this NN,  \n",
    "  \n",
    "$z^{[1]}=w^{[1]}x+b^{[1]} \\ \\ \\  , \\ a^{[1]}=g^{[1]}(z^{[1]})$  \n",
    "$z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}, \\ a^{[2]}=g^{[2]}(z^{[2]})$  \n",
    "$z^{[3]}=w^{[3]}a^{[2]}+b^{[3]}, \\ a^{[3]}=g^{[3]}(z^{[3]})$  \n",
    "$z^{[4]}=w^{[4]}a^{[3]}+b^{[4]}, \\ a^{[4]}=g^{[4]}(z^{[4]})=\\hat{y}$  \n",
    "\n",
    "In general:  \n",
    "  \n",
    "$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}, \\ a^{[l]}=g^{[l]}(z^{[l]})$  \n",
    "\n",
    "Vectorization:  \n",
    "  \n",
    "$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}, \\ A^{[l]}=g^{[l]}(Z^{[l]})$  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting your Matrix Dimensions Right  \n",
    "Here is the script used to ensure the matrix dimensions are right:  \n",
    "<img src=\"images/new5layer.png\" width=\"400\"><img src=\"images/dimensionscript.png\" width=\"300\">   \n",
    "In general:  \n",
    "  \n",
    "$w^{[l]}, dw^{[l]}: \\ (n^{[l]},n^{[l-1]})$  \n",
    "$b^{[l]},db^{[l]}\\ \\ \\ : \\ (n^{[l]},1)$  \n",
    "$a^{[l]},z^{[l]}\\quad : \\ (n^{[l]},1)$  \n",
    "  \n",
    "Vectorization:  \n",
    "  \n",
    "$A^{[l]},Z^{[l]}\\ \\ \\quad : \\ (n^{[l]},m)$  \n",
    "$dA^{[l]},dZ^{[l]}\\ \\  : \\ (n^{[l]},m)$  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Deep Representaions?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward Propagation\n",
    "\n",
    "### blocks  \n",
    "\n",
    "<img src=\"images/blocks.png\" width=\"900\">\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computation  \n",
    "\n",
    "**Forward propagation** for layer $l$:  \n",
    "  \n",
    "$\\text{input} \\ a^{[l-1]} \\textcolor{red}{\\Rightarrow} z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]} \\textcolor{red}{\\Rightarrow} a^{[l]}=g^{[l]}(z^{[l]})$.  \n",
    "OUTPUT: $\\textcolor{red}{a^{[l]}, \\ \\text{cache}(z^{[l]})}$.   \n",
    "The \"cache\" is used in our implementation to store values computed during forward propagation to be used in backward propagation.  \n",
    "In the vectorization way, it is:  \n",
    "$\\text{input} \\ A^{[l-1]} \\textcolor{red}{\\Rightarrow} Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} \\textcolor{red}{\\Rightarrow} A^{[l]}=g^{[l]}(Z^{[l]})$  \n",
    "  \n",
    "**Backward propagation** for layer $l$:  \n",
    "$\\text{input} \\ da^{[l]} \\textcolor{red}{\\Rightarrow} dz^{[l]}=da^{[l]}*g^{\\prime [l]}(z^{[l]}) \\textcolor{red} {\\Rightarrow} dw^{[l]}=dz^{[l]} \\cdot a^{[l-1]T} \\textcolor{red}{\\Rightarrow} db^{[l]}=dz^{[l]} \\textcolor{red}{\\Rightarrow} da^{[l-1]}=w^{[l]T} \\cdot dz^{[l]}$.  \n",
    "OUTPUT: $\\textcolor{red}{da^{[l-1]}, \\ dW^{[l]}, \\ db^{[l]}}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks in Depth\n",
    "\n",
    "Here are some detailed math calculations. \n",
    "I referred to [these articles posted by jonaslali](https://community.deeplearning.ai/t/feedforward-neural-networks-in-depth/98811), but rewrote to the format that I am comfortable with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Vs. Hyperparameters\n",
    "\n",
    "### what is hyperparameter?  \n",
    "\n",
    "Hyperparameters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning. The prefix ‘hyper_’ suggests that they are ‘top-level’ parameters that control the learning process and the model parameters that result from it [@https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac].  \n",
    "Learning rate $\\alpha$, number of iterations, number of hidden layers, number of hidden units, choice of activation functions are all hyperparameters."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
